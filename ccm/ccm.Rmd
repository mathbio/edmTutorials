---
title: "Convergent Cross Mapping (CCM)"
subtitle: "School on Physics Applications in Biology, January 2018"
author: "Brenno Cabella, Roberto Kraenkel, Renato Coutinho, Paulo Inácio Prado, Rafael Lopes"
date: "`r format(Sys.time(), '%d de %B de %Y')`"
output:
         rmdformats::readthedown:
         self_contained: true
         thumbnails: true
         lightbox: true
         gallery: false
         highlight: tango
         toc_depth: 4
---

```{r setup, echo=FALSE, warning=FALSE, message=F}
library(knitr)#; library(zoo); library(xts)
library(plotly)
library(rEDM)
library(ggplot2)#; library(cowplot)
library(dplyr); library(tidyr)


opts_chunk$set(fig.align = 'center',
               fig.show = 'hold',
               fig.height = 5,
               warning = FALSE, message = FALSE, error = FALSE, echo=FALSE)
options(formatR.arrow = TRUE,width = 90)###, cache=TRUE)
```

This tutorial presents the general idea of Convergent Cross Mapping (CCM, Ye *et al.* 2017), then it shows some practical examples using syntetic data, and lastly we propose a real data analysis as exercise.
For these activities, you will need the most recent version of [R](https://cran.r-project.org/) and the rEDM package installed in your working computer.

Before you proceed,
make sure you have read the introduction and the first section (*"Empirical Dynamic Modelling"*) of the
rEDM's tutorial which you can find [here](https://cran.r-project.org/web/packages/rEDM/vignettes/rEDM-tutorial.html) 
or in your local R installation with the command

# Convergent Cross Mapping: From Chaos to Chaos-ality

## General idea [^1]

One of the corollaries to the Generalized Takens’ Theorem is that it should be possible to cross predict or cross map between variables that are observed from the same dynamical system. Consider two variables, x and y, that interact in a dynamic system. Then the univariate reconstructions based on x alone should uniquely identify the system state and thus the corresponding value of y, (and vice versa).

## Syntetic Data

First, let's simulate a deterministic discrete-time dynamics with chaotic behavior.
To create a 150-step time series following this dynamics, run the commands below in the R console:

```{r generate data, echo=TRUE}

## Two vectors to store data
X <- c()
Y <- c()
## Initial values
X[1] <- 0.1
Y[1] <- 0.1
## Iterate the dynamics 150 time steps
for(i in 2:150){
  X[i] <- 3.77*X[i-1]*(1-X[i-1])
  Y[i] <- 3.82*Y[i-1]*(1-Y[i-1]-0.05*X[i-1])
}
XY<-as.data.frame(cbind(X,Y))
  
```
Note that the variable X is causing changes in Y. 
However, if we plot the X and Y time series of this chaotic system, they do not seem to be related:

```{r plot 1st time series}
par(cex=1.1,lwd=2)
  plot(20:50,X[20:50],type="b", pch=18, col="blue",ylim=c(min(X,Y),max(X,Y)),main='Two Species',xlab = 'time',ylab='Population')
  lines(20:50,Y[20:50],pch=19, col="red", type="b",lty=2,lwd=2)
  legend(x = "bottomright", legend = c("X", "Y"),lty=c(1,2),pch=c(18,19) ,col = c("blue", "red"), inset = 0.02,lwd=2)
  # save data for future use
  write.csv(XY, file = "data_unidirectional.csv",row.names=F)
  
  
```

X and Y do not seem to be related, although we know that X and Y are coupled (it's a syntetic data!).
Moreover, we can calculate the correlation coeficient between X and Y:

```{r plot correlation}
fit<-lm(Y ~ X)
plot(X,Y,main='Correlation (X,Y)')
abline(fit$coefficients[1],fit$coefficients[2])
legend(x = "bottomleft", legend = paste('r =',round(cor(X,Y)*100)/100),inset = 0.02,col = 'black',lty = 1)

```

The results show that X and Y are not correlated, even though the data comes from a model with known causality (*i.e.* X is causing changes in Y). 
This is a clear example that "lack of correlation does not imply lack of causation".

## Cross Mapping[^2]

How can we then extract the causality of X in Y from their dynamics (time-series)?
As we have seen in previous sections, a generic property of lagged-coordinate embedding is that the states of X(t) on the lagged reconstruction map (Mx) maps one-to-one to states on the original attractor manifold M, and local neighborhoods on Mx map to local neighborhoods on M. 

It follows that for two variables X and Y that are dynamically coupled, local neighborhoods on their lagged reconstructions (Mx and My, respectively) will map to each other since X and Y are essentially alternative observations of the common original attractor manifold M.

*Convergent* cross mapping determines how well local neighborhoods on Mx correspond to local neighborhoods on My and vice versa. 
To do so, a manifold Mx is constructed from lags of the variable X and used to estimate **contemporaneous (Marina: you mean simultaneous in time? Is it really at the same t?)** states of Y. 
Similarly, a manifold My is constructed from lags of variable Y and used to estimate **contemporaneous** states of X.

To do so, we first need to obtain the optimal embedding dimension for both variables using the simplex function **(see tutorial XXX if you have doubts about optimal embedding)**.


```{r optimal embeddings X, echo=T}
options(warn = -1)
simplex_X<-simplex(X,silent=T)
plot(simplex_X$rho,type='o', ylab = "Forecast Skill (rho)", xlab="Embedding Dimension (E)")
E_star_X<-which.max(simplex_X$rho)
print(paste('E*(X) =',E_star_X))
```

```{r optimal embeddings Y, echo=T}
simplex_Y<-simplex(Y,silent=T)
plot(simplex_Y$rho,type='o', ylab = "Forecast Skill (rho)", xlab="Embedding Dimension (E)")
E_star_Y<-which.max(simplex_Y$rho)
print(paste('E*(Y) =',E_star_Y))

```

The optimal embedding dimensions for X and Y are equal to 2.

## Cross-mapping from Mx to My (X_xmap_Y)

Now that we have the optimal embeddings, we can construct the shadow manifolds Mx and My using the make_block function from the rEDM package. Use the command below to load this function into your R workspace (it will be included in rEDM itself in future releases of the package):

```{r load make_block, eval=TRUE, echo=FALSE}
source("https://raw.githubusercontent.com/mathbio/edmTutorials/master/utilities/make_block.R")
```

Now let's create the shadow manifold:

```{r make_block, echo=T}

Shadow_MXY<-make_block(XY,max_lag = 2) # max_lag is the optimal embedding dimension
Shadow_MX<-Shadow_MXY[,2:3]
Shadow_MY<-Shadow_MXY[,4:5]

head(Shadow_MXY)

```



To better understand the cross-mapping, lets start by performing one single prediction using the index of a single predictor.

```{r MX X_xmap_Y_code, echo=T}

predictor<-70

```

Here, we are using the therm "prediction", but instead of predicting future values of X, we will "predict values of Y(t) using X(t) and vice versa.
This "cross-prediction" is performed between different variables but for the same point in time.

Next, we obtain the indexes of the E*+1 nearest neighbors from the given predictor generating the simplex_Mx.

```{r neighbors_X, echo=T}
dist.matrix_X <- as.matrix(dist(Shadow_MX, upper=TRUE))
neigb_X <- order(dist.matrix_X[predictor,])[2:4]
neigh_X_print<-c(neigb_X)
print(paste('simplex_Mx:',list(neigh_X_print)))
```

The following plot presents Mx with the predictor (red dot) and respective simplex_Mx (blue dots).
```{r MX X_xmap_Y, echo=FALSE, fig.align='center'}
p_MX_X_to_Y <- plot_ly(Shadow_MX, x=~X, y=~X_1, marker=(list(color=grey)), opacity=0.25) %>%
  layout(xaxis = list(title = 'X'),yaxis = list(title = 'X(t-1)'),title='Mx') %>%
  add_markers(text = paste("time =",1:length(X)), showlegend = FALSE) %>%
  add_trace( x = ~X, y=~X_1,data=Shadow_MX[c(predictor,neigb_X),],opacity=1,marker=list(color=c("red","blue","blue","blue")),type="scatter", mode="markers",text = paste("time =",c(predictor,neigb_X)))
p_MX_X_to_Y
```

The cross-mapping process starts by finding (mapping) the simplex_Mx in My, creating the simplex_My.
Note that simplex_My has the same indexes as simplex_Mx.
The figure below shows My and simplex_My (green dots).
```{r MY_X_xmap_Y}
p_MY_X_to_Y <- plot_ly(Shadow_MY, x=~Y, y=~Y_1, marker=(list(color=grey)), opacity=0.25) %>%
  layout(xaxis = list(title = 'Y'),yaxis = list(title = 'Y (t-1)'),title='My') %>%
  add_markers(text = paste("time =",1:length(Y)), showlegend = FALSE) %>%
  add_trace( x = ~Y, y=~Y_1,data=Shadow_MY[c(neigb_X),],opacity=1,marker=list(color=c("green","green","green")),type="scatter", mode="markers",text = paste("time =",c(neigb_X)))
p_MY_X_to_Y

```
The simplex(My) will then be used to estimate the contemporaneous value of the predictor in My, obtaining the predicted value Y(tilda). 
```{r estimating Y from X (X_xmap_Y), echo=T}
lib<-lib <- c(1, NROW(Shadow_MXY))
block_lnlp_output_XY <- block_lnlp(Shadow_MXY, lib = lib, pred = lib, columns = c("X",
 "X_1"), target_column = "Y", stats_only = FALSE, first_column_time = TRUE)
observed_all_Y <- block_lnlp_output_XY$model_output[[1]]$obs
predicted_all_Y <- block_lnlp_output_XY$model_output[[1]]$pred
pred_obs_Y<-as.data.frame(cbind(predicted_all_Y,observed_all_Y))
colnames(pred_obs_Y)<-c('Predicted Y','Observed Y')
head(pred_obs_Y)
```

Taking every prediction in the series will provides us with many comparisons between predicted and observed.
This is better vizualized in the plot below.
The red dot represents the predictor used in the example above.

```{r plot_obs_pred_MX_MY}
fit_YX<-lm(predicted_all_Y ~ observed_all_Y)
plot_range <- range(c(observed_all_Y, predicted_all_Y), na.rm = TRUE)
plot(observed_all_Y,predicted_all_Y, xlim = plot_range, ylim = plot_range, xlab = "Observed",
ylab = "Predicted")
abline(fit_YX$coefficients[1],fit_YX$coefficients[2])
legend(x = "bottomright", legend = paste('r =',round(cor(observed_all_Y, predicted_all_Y)*100)/100),inset = 0.02,col = 'black',lty = 1)
observed_pred_Y<-observed_all_Y[predictor-2]
predicted_pred_Y<-predicted_all_Y[predictor-2]
points(observed_pred_Y,predicted_pred_Y,col='red',pch=16,cex=1.2)
```





## Cross-mapping from My to Mx 

Similarly to the previous mapping, we obtain the indexes of the E*+1 nearest neighbors from the given predictor. 
However, since we are cross-mapping from My to Mx, the simplex_My is generate in shadow maninfold My.

```{r neighbors_y, echo=T}
dist.matrix_Y <- as.matrix(dist(Shadow_MY, upper=TRUE))
neigb_Y <- order(dist.matrix_Y[predictor,])[2:4]
neigh_Y_print<-c(neigb_Y)
print(paste('simplex_My:',list(neigh_Y_print)))
```

The following plot presents My with the predictor (red dot) and respective simplex_My (blue dots).
```{r MY Y_xmap_X, echo=FALSE, fig.align='center'}
p_MY_Y_to_X <- plot_ly(Shadow_MY, x=~Y, y=~Y_1, marker=(list(color=grey)), opacity=0.25) %>%
  layout(xaxis = list(title = 'Y'),yaxis = list(title = 'Y(t-1)'),title='My') %>%
  add_markers(text = paste("time =",1:length(X)), showlegend = FALSE) %>%
  add_trace( x = ~Y, y=~Y_1,data=Shadow_MY[c(predictor,neigb_Y),],opacity=1,marker=list(color=c("red","blue","blue","blue")),type="scatter", mode="markers",text = paste("time =",c(predictor,neigb_Y)))
p_MY_Y_to_X
```

Next, we map the simplex_My in Mx, creating the simplex_Mx.
Analogously, simplex_Mx has the same indexes as simplex_My.
The figure below shows Mx and simplex_Mx (green dots).
```{r MX_Y_xmap_X}
p_MX_Y_to_X <- plot_ly(Shadow_MX, x=~X, y=~X_1, marker=(list(color=grey)), opacity=0.25) %>%
  layout(xaxis = list(title = 'X'),yaxis = list(title = 'X (t-1)'),title='Mx') %>%
  add_markers(text = paste("time =",1:length(Y)), showlegend = FALSE) %>%
  add_trace( x = ~X, y=~X_1,data=Shadow_MX[c(neigb_Y),],opacity=1,marker=list(color=c("green","green","green")),type="scatter", mode="markers",text = paste("time =",c(neigb_Y)))
p_MX_Y_to_X

```

The simplex(Mx) will then be used to estimate the contemporaneous value of the predictor in Mx, obtaining the predicted value X(tilda). 
```{r estimating X from Y (Y_xmap_X), echo=T}
lib<-lib <- c(1, NROW(Shadow_MXY))
block_lnlp_output_YX <- block_lnlp(Shadow_MXY, lib = lib, pred = lib, columns = c("Y",
 "Y_1"), target_column = "X", stats_only = FALSE, first_column_time = TRUE)
observed_all_X <- block_lnlp_output_YX$model_output[[1]]$obs
predicted_all_X <- block_lnlp_output_YX$model_output[[1]]$pred
pred_obs_X<-as.data.frame(cbind(predicted_all_X,observed_all_X))
colnames(pred_obs_X)<-c('Predicted X','Observed X')
head(pred_obs_X)

```

Below is the plot for all predictions and observations as well as the Pearson's correlation coefficient.
The red dot represents the predictor used in the example above.

```{r plot_obs_pred_MY_MX}
fit_XY<-lm(predicted_all_X ~ observed_all_X)
plot_range <- range(c(observed_all_X, predicted_all_X), na.rm = TRUE)
plot(observed_all_X, predicted_all_X, xlim = plot_range, ylim = plot_range, xlab = "Observed",
ylab = "Predicted")
abline(fit_XY$coefficients[1],fit_XY$coefficients[2])
legend(x = "bottomright", legend = paste('r =',round(cor(observed_all_X,predicted_all_X)*100)/100),inset = 0.02,col = 'black',lty = 1)
observed_pred_X<-observed_all_X[predictor-2]
predicted_pred_X<-predicted_all_X[predictor-2]
points(observed_pred_X,predicted_pred_X,col='red',pch=16,cex=1.2)

```


# What does it all mean?

From the previous results, if we use Y to estimate X we obtain good predictions.
Since there is information about X inside Y, we can use Y to estimate X.
In another words, if X causes Y, the cross mapping skill from My to Mx will generally gives us good results (high correlations).
Important to note here is the inverse relation between cross mapping and causality.

## Convergent part of CCM

Convergence means that cross-mapped estimates improve in estimation skill with time-series length L (sample size used to construct a library).



```{r convergent}
  # cross map from X to Y
  X_xmap_Y<- ccm(XY, E = 2, lib_column = "X", target_column = "Y", lib_sizes = seq(10, 130, by = 10), num_samples = 100, random_libs = TRUE, replace = TRUE)
  # cross map from Y to X
  Y_xmap_X<- ccm(XY, E = 2, lib_column = "Y", target_column = "X", lib_sizes = seq(10, 130, by = 10), num_samples = 100, random_libs = TRUE, replace = TRUE)
  
  #mean values
  X_xmap_Y_means <- ccm_means(X_xmap_Y)
  Y_xmap_X_means <- ccm_means(Y_xmap_X)
  
  #plot graphs
  plot(X_xmap_Y_means$lib_size, pmax(0, X_xmap_Y_means$rho), type = "l", col = "red",main='Two Species', xlab = "Library Size", ylab = "Cross Map Skill (rho)", ylim = c(0,1))
  lines(Y_xmap_X_means$lib_size, pmax(0, Y_xmap_X_means$rho), col = "blue")
  legend(x = "topleft", legend = c("X_xmap_Y", "Y_xmap_X"), col = c("red", "blue"), cex=1.1,lwd=2, inset = 0.02)
  
```

With more data, the trajectories defining the attractor fill in, resulting in closer nearest neighbors and declining estimation error (a higher correlation coefficient) as L increases. Thus, CCM becomes a necessary condition for causation.


# Exercises

(1) What happens when there is a invertion in cause-effect realation? Change the model so the Y variable is causing changes in X.

(2) What if there is no interaction between X and Y?

(3) What about both variables interacting to each other?

(4) Identifying causal networks is important for effective policy and management recommendations on climate, epidemiology, financial regulation, and much else. 
In the following exercise, you should use CCM to identify causality between anchovy landings in California and Newport Pier sea-surface temperature.

```{r ex3}
data(sardine_anchovy_sst)
```

# Learn more

* Sugihara G. and R. M. May. 1990. Nonlinear forecasting as a way of distinguishing chaos 
from measurement error in time series. Nature 344:734–741.
* Sugihara G. 1994. Nonlinear forecasting for the classification of natural time series. 
Philosophical Transactions: Physical Sciences and Engineering 348:477–495.
* Anderson C. & Sugihara G. Simplex projection - Order out the chaos. http://deepeco.ucsd.edu/simplex/
* ["Simplex projection walkthrough"](http://opetchey.github.io/RREEBES/Sugihara_and_May_1990_Nature/Simplex_projection_walkthrough.html), a tutorial by Owen Petchey.


[^1]: Taken from [rEDM vignette(https://cran.r-project.org/web/packages/rEDM/vignettes/rEDM-tutorial.html) section (*"Causality Inference and Cross Mapping"*) 

[^2]: This section is adapted from: Sugihara et al., Detecting Causality in Complex Ecosystems (Supplementary Materials), Science, 2012.
